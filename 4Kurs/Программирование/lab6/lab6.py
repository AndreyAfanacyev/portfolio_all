# -*- coding: utf-8 -*-
"""lab6.ipynb

Automatically generated by Colaboratory.

# Обработка текста на Python / NLP with Python

# Установка зависимостей в колабе
"""

# Commented out IPython magic to ensure Python compatibility.
# %%sh
# 
# pip install yargy # для обработки текста 
# pip install natasha # для обработки текста 
# pip install wordcloud # для создания изображения с облаком тегов

"""## Парсинг и обработка информации со страниц сайта

Можно использовать механизм, используемый в прошлой ЛР (BeautifulSoup) или что-то другое.

Предлагается использовать регулярные выражения, поскольку все новости на сайте ведут на подраздел ```/news/*```

Извлекаем ссылки на конкретные новости и открываем каждую страницу аналогичным образом получаем текст новости полностью.

Не забыть выполнить предварительную очистку и токенизацию текста по словам. После очистки текста требуется положить его в отдельный текстовый файл или сохранить в строковую переменную. В конце — склеить весь текст в единый текстовый файл. 

Затем, используя [библиотеку natasha](https://natasha.github.io/ner/) извлекаем имена упоминаемых в тексте людей и добавляем их в json-файл. 
Далее, для имён требуется сериализовать данные из json-файла, убрав имена и оставив только фамилии. После этого сделать из фамилий текст и визуализировать информацию о статистики встречаемости в виде облака тегов. 
Аналогично, сделать облако тегов для ключевых понятий. 
"""

import re
import requests
from bs4 import BeautifulSoup

html = requests.get('https://old.herzen.spb.ru/main/news/').text # получение исходного текста страницы

r = r'\/news\/\d{2}-\d{2}-\d{4}(?:_\d+)?\/' # регулярное выражение для поиска всех ссылок
# формат ссылок - /news/dd/mm/yyyy/, /news/dd-mm-yyyy_x/
links = re.findall(r, html, re.MULTILINE)

f = open('text.txt', 'w') # сохраняем текст новостей в файл

for link in links[:25]: # выбираем часть ссылок, т.к. их много
    html = requests.get('https://old.herzen.spb.ru' + link).text # получение исходного кода страницы
    bs = BeautifulSoup(html, "html.parser") # инициализация BeautifulSoup
    td = bs.findAll('td', {'class': 'blockwh'})[0] # выбираем ячейку таблицы
    block = td.findAll('div')[-1] # выбираем блок с текстом новости
    block_text = block.text # получаем текст
    block_text = block_text.strip() # удаление пробельных символов
    f.write(block_text + "\n") # запись текста в файл

f.close()

import nltk # библиотека для обработки естественного языка
nltk.download('stopwords') # загружаем список стоп-слов

from nltk.corpus import stopwords

from natasha import (
    Segmenter,
    MorphVocab,
    NewsEmbedding,
    NewsMorphTagger,
    NewsSyntaxParser,
    NewsNERTagger,
    PER,
    NamesExtractor,
    Doc
)

# чтение текстов новостей из файла
f = open('text.txt')
text = f.read()
f.close()

text = text.replace('им. А. И. Герцена', '') # все новости относятся к университету, нет смысла добавлять в облако слов

segmenter = Segmenter() # сегментация текста
morph_vocab = MorphVocab() # анализ морфологии

emb = NewsEmbedding() # эмбеддинги текстов новостей
morph_tagger = NewsMorphTagger(emb) # анализ морфологии
syntax_parser = NewsSyntaxParser(emb) # синтаксический анализ
ner_tagger = NewsNERTagger(emb) # извлечение именованных сущностей

names_extractor = NamesExtractor(morph_vocab) # извлечение имён из текста

doc = Doc(text)
doc.segment(segmenter) # сегментация текста
doc.tag_morph(morph_tagger) # анализ морфологии
doc.parse_syntax(syntax_parser) # синтаксический анализ

doc.tag_ner(ner_tagger) # извлечение именованных сущностей

names = [] # список имён

for span in doc.spans: # перебор всех фраз в тексте
    span.normalize(morph_vocab) # нормализация (нормальная форма слова)
    if span.type == PER: # имя собственное
        span.extract_fact(names_extractor) # извлечение имени
        names.append(span.normal) # добавление имени в нормальной форме в список


words = [] # список слов

stopwords_russian = stopwords.words('russian')
punctuation_symbols = '.,:;!?"«»()/\—+-'

for token in doc.tokens: # перебор всех токенов в тексте
    token.lemmatize(morph_vocab) # лемматизация
    if token.lemma not in stopwords_russian and token.lemma not in punctuation_symbols: # токен не входит в список стоп-слов и не является знаком препинания
        words.append(token.lemma) # добавление в список

"""# Построение изображения с облаком тегов на Python"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

from collections import Counter

from wordcloud import WordCloud # библиотека для построения облака слов
import matplotlib.pyplot as plt

names_dict = Counter(names) # подсчёт кол-ва повторений
# построение облака слов
wordcloud = WordCloud().generate_from_frequencies(names_dict)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")

words_dict = Counter(words) # подсчёт кол-ва повторений
# построение облака слов
wordcloud = WordCloud().generate_from_frequencies(words_dict)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
