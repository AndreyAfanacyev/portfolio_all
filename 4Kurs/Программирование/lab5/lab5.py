# -*- coding: utf-8 -*-
"""lab5.ipynb

Automatically generated by Colaboratory.
"""

from collections import Counter

import requests
import nltk

# загрузка текста
res = requests.get('https://gist.githubusercontent.com/nzhukov/b66c831ea88b4e5c4a044c952fb3e1ae/raw/7935e52297e2e85933e41d1fd16ed529f1e689f5/A%20Brief%20History%20of%20the%20Web.txt')
text = res.text

# загрузка дополнений для nltk
nltk.download('punkt') # токенизация текста
nltk.download('averaged_perceptron_tagger') # частеречная разметка

tokens = nltk.word_tokenize(text) # токенизация текста
print(tokens)

tags = nltk.pos_tag(tokens) # частеречная разметка
print(tags) # список пар слово - часть речи

parts = [part for word, part in tags] # получение списка частей речи

counter = Counter(parts) # подсчёт наиболее часто встречаемых в тексте частей речи
for part, value in counter.most_common(5):
    print(f'{part} - {value}')

# в тегах учитывается время глагола, число и т.д.
# можно сгруппировать теги, оставив первые 2 символа
# список тегов: https://stackoverflow.com/questions/15388831/what-are-all-possible-pos-tags-of-nltk
parts = [part[:2] for word, part in tags] # оставляем 2 символа

names = {
    'NN': 'nouns (существительные)',
    'VB': 'verbs (глаголы)',
    'IN': 'prepositions (предлоги)',
    'DT': 'determiners (детерминативы)', # the, some, this, all, any и т.д.
    'JJ': 'adjectives (наречия)'
}

counter = Counter(parts) # подсчёт наиболее часто встречаемых в тексте частей речи
for part, value in counter.most_common(5):
    print(f'{names[part]} - {value}')
